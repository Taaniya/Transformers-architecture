{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCiW+hoiWcwIqBAX3+TlFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taaniya/Transformers-architecture/blob/main/Explore_outputs_in_causal_attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below function applies attention mechanism for encoder / decoder block in a transformer architecture, given query, key and value vectors and an appropriate masks (attention / padding)\n",
        "\n",
        "For ease of understanding we'll explore the output without passing mask.\n",
        "\n",
        "Note that the attention mask is only used for an input to a decoder layer, and not in the encoder layer. This is also called as look_ahead_mask as it masks the tokens ahead of the current token.\n",
        "\n",
        "This function is used in Multi-head attention attention mechanism in both Encoder and Decoder blocks"
      ],
      "metadata": {
        "id": "OPd-V_Cnmst3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "1OgM7bO1rfkP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qyq6FA5FlTtT"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "  q: query shape = (..., seq_len_q, depth)\n",
        "  k: key shape = (..., seq_len_k, depth)\n",
        "  v: value shape = (..., seq_len_v, depth_v)\n",
        "  mask: float tensor with shape broadcastable to (..., seq_len_q, seq_len_k). Defaults to None\n",
        "\n",
        "  Returns:\n",
        "  output, attention_weights\n",
        "  \"\"\"\n",
        "  matmul_qk = tf.matmul(q,k, transpose_b=True)         # (..., seq_len_q, seq_len_k)\n",
        "  # print(f\"matmul_qk: \\n {matmul_qk}\")\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "  # print(f\"scaled matmul_qk: \\n {scaled_attention_logits}\")\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9) \n",
        "\n",
        "  print(f\"scaled_attention_logits: \\n {scaled_attention_logits}\")\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)     # (..., seq_len_q, seq_len_k)\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)   # Set diagonal and all subdiagonals to zero, rest to 1 \n",
        "  return mask"
      ],
      "metadata": {
        "id": "60T8GmNVrZu2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example, assume three token prefix received by decoder\n",
        "\n",
        "prefix =  tf.constant([[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [2.0, 3.0, 4.0, 5.0]])"
      ],
      "metadata": {
        "id": "bPn65URBraFI"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZjbunLirb99",
        "outputId": "e779b671-5ee1-43ba-ea0d-d96e6e1405fb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = create_look_ahead_mask(prefix.shape[0])\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIfEHS--rksL",
        "outputId": "8826cb31-df97-428f-af88-73106a757402"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "causal_attn_output, attn_weights = scaled_dot_product_attention(prefix, prefix, prefix, mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbVFkmEwrkop",
        "outputId": "49eae10a-ad68-4ac2-d070-ffd4f2390ee7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaled_attention_logits: \n",
            " [[ 1.5000000e+01 -9.9999994e+08 -1.0000000e+09]\n",
            " [ 3.5000000e+01  8.7000000e+01 -9.9999994e+08]\n",
            " [ 2.0000000e+01  4.8000000e+01  2.7000000e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "causal_attn_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utpF7yJjrkk4",
        "outputId": "129feb18-a99c-4dbc-9f06-da829a107822"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[1., 2., 3., 4.],\n",
              "       [5., 6., 7., 8.],\n",
              "       [5., 6., 7., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV3mY2MqrkhE",
        "outputId": "abe9a73a-f46d-4533-9dca-e66b5311a70b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
              "       [2.6102792e-23, 1.0000000e+00, 0.0000000e+00],\n",
              "       [6.9143996e-13, 1.0000000e+00, 7.5825607e-10]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in the above matrix signifies a token. For each token, how much attention to pay for itself and its past tokens. \n",
        "Values in column indicates the amount of attention / weight to apply for other tokens in the given sequence.\n",
        "Column 1, token 1, column 2 -> token 2 , column 3 -> token 3."
      ],
      "metadata": {
        "id": "8QAKNBDox1IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.matmul(attn_weights, prefix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPlNb57QruRv",
        "outputId": "de330cdb-b4fa-4e66-8bdf-5a441d42e1d5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
              "array([[1., 2., 3., 4.],\n",
              "       [5., 6., 7., 8.],\n",
              "       [5., 6., 7., 8.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See that the 1st and 2nd tokens only look at its previous tokens, and lose on information in the next tokens since there'no attention paid to them. \n",
        "\n",
        "This techniques works well during training to avoid the model attending to future tokens, but poses a limitation during inference, when a sequence is passed to the Decoder , which only creates representations of each token in given sequence based only on tokens only preceding it."
      ],
      "metadata": {
        "id": "tpUVzv601wid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### References \n",
        "* [Unified Language Model Pre-training for Natural Language Understanding and Generation, by Dong et al, 2019](https://arxiv.org/pdf/1905.03197.pdf)\n",
        "* [Exploring the limits of Transfer Learning with a Unified\n",
        "Text-to-Text Transformer, 2020](https://arxiv.org/pdf/1910.10683.pdf)"
      ],
      "metadata": {
        "id": "WzdhRsO32yYT"
      }
    }
  ]
}